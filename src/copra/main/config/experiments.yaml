defaults:
  # - benchmark: stack_machine
  # - eval_settings: n_60_dfs_gpt35_always_retrieve_no_ex
  # - env_settings: bm25_retrieval_only_local_no_dfns
  # - prompt_settings: coq_dfs_always_retrieve_stack_machine
  # - override hydra/job_logging: 'disabled'
  # - benchmark: miniF2F_test
  # - eval_settings: n_60_dfs_gpt4_128k_no_retrieve_no_ex
  # - prompt_settings: lean_few_shot_informal_to_formal_dfs_gpt4_turbo
  - env_settings: bm25_retrieval
  - benchmark: simple_benchmark_lean4 # miniF2F_test_easy_to_hard # miniF2F_test_isabelle_easy_to_hard # miniF2F_test
  - eval_settings: n_60_dfs_gpt4_o_no_retrieve_no_ex # n_4_few_gpt4_turbo # n_60_dfs_gpt4_128k_always_retrieve_no_ex # n_4_few_gpt4_turbo # n_60_dfs_gpt4_128k_no_retrieve_no_ex # n_60_hammer_no_retrieve_no_ex # n_60_dfs_gpt35_no_retrieve_no_ex
  - prompt_settings: lean4_dfs # lean4_few_shot # lean4_dfs # isabelle_dfs_hammer # isabelle_dfs_hammer
  - override hydra/job_logging: 'disabled'

eval_settings:
 timeout_in_secs: 200
 proof_retries: 4
 temperature: 0.7

# prompt_settings:
#   # Informal proof human written
#   informal_proof_repo: data/test/informal_lean_proj
#   # Informal proof gpt35
#   informal_proof_file: .log/proofs/eval_driver/informal_few_shot/miniF2F_test/20231204-233231/informal_proofs

# To run this experiment, execute the following command:

# Few shot Lean
# nohup python src/main/eval_benchmark.py prompt_settings=lean_few_shot env_settings=bm25_retrieval eval_settings=n_4_few_gpt35 benchmark=simple_benchmark_lean  &

# Dfs Agent Lean
# nohup python src/main/eval_benchmark.py prompt_settings=lean_dfs env_settings=bm25_retrieval eval_settings=n_60_dfs_gpt35_always_retrieve_no_ex benchmark=simple_benchmark_lean  &

# Few shot Coq
# nohup python src/main/eval_benchmark.py prompt_settings=coq_few_shot env_settings=bm25_retrieval eval_settings=n_4_few_gpt35 benchmark=simple_benchmark_1  &

# Dfs Agent Coq
# nohup python src/main/eval_benchmark.py prompt_settings=coq_dfs_always_retrieve env_settings=bm25_retrieval eval_settings=n_60_dfs_gpt35_always_retrieve_no_ex benchmark=simple_benchmark_1  &

# Few shot Isabelle
# nohup python src/main/eval_benchmark.py prompt_settings=isabelle_few_shot_hammer env_settings=bm25_retrieval eval_settings=n_4_few_gpt35 benchmark=simple_benchmark_isabelle  &

# Dfs Agent Isabelle
# nohup python src/main/eval_benchmark.py prompt_settings=isabelle_dfs_hammer env_settings=bm25_retrieval eval_settings=n_60_dfs_gpt35_always_retrieve_no_ex benchmark=simple_benchmark_isabelle  &

# Dfs Agent Lean4